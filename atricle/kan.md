# KAN网络

## MLP本质回顾

MLP本质上是用一个线性模型外面包了一层非线性激活函数来实现非线性空间变换。

**MLP的硬伤**

1. 梯度消失和梯度爆炸：反向传播计算梯度时
2. 参数效率低：MLP通常使用全连接层，导致参数数量迅速怎加。不仅 增加了计算负担，也增加了模型过拟合的风险。这就是大模型的困境，拼参数量没出路，大部分学习都是浪费的。
3. 处理高维数据的能力有限：MLP没有利用数据的内在结构。例如再图像处理中，MLP无法有效利用像素之间的局部空间关联，这使得其再图像识别等任务上的性能不如卷积神经网络（CNN）
4. 长期依赖问题：MLP理论上可逼近任何函数，但再实际应用中很难捕捉到输入序列中长期依赖关系。而RNN和Transformer在这些任务中表现通常更好

但无论CNN/RNN/Transformer怎么改进，都躲不了MLP这个基础模型的硬伤，就是这个线性组合+激活函数的模式。

## KAN网络

Kolmogorov-Arnold Networks，基于Kolmogorov-Arnold表示定理。是这两位俄罗斯数学家提出的如何用一组叫简单的函数来表示任何一个多变量的连续函数。
$$
f(X) = \sum_{q=1}^{2n+1}\Phi_q(\sum_p^n\phi_{q,p}(x_p))
$$
输入是x，$\phi_{q,p}(x_p)$是基本的一元函数，$\Phi_q$是外层的函数，各自接收内层求和结果作为输入。外层的求和表示整个函数f(X)是子函数$\Phi_q$的和。

用图来表示就相当于一个两层的神经网络，区别在于：

1. 没了线性组合，而是直接对输入进行激活。
2. 这些激活函数不是固定的，而是可以学习的。

![](./assets/2024-05-16%20233022.png)

所有的非线性函数$\Phi$​都采用同样的函数结果（文章使用了数值分析中的样条函数spline），只是用不同的参数控制其形状。对比MLP和KAN，最大的区别就是变固定的非线性激活+线性参数学习为直接对参数化的非线性激活函数的学习。

### 如何构建一个深层的KAN网络

KAN网络对原始公式进行了修改，没有严格按照2n+1的宽度构建网络，减少了复杂度。原始的两层KAN网络的形状是[n, 2n + 1, 1]，现在变成了多层级联，放开了2n+1的结构限制，隐层节点数可以自由发挥。

![](./assets/2024-05-16%20233432.png)

#### 如何提升准确性

**MLPs**通过增加模型的宽度和深度可以提高性能，但这种方法效率低下，因为需要独立地训练不同大小的模型。**KANs**：开始可以用较少的参数训练，然后通过简单地细化其样条网格来增加参数，无需重新训练整个模型。

基本原理就是通过将样条函数（splines）旧的粗网格转换为更细的网格，并对应地调整参数，无需从头开始训练就能扩展现有的KAN模型。这种技术成为“网格扩展”（grid extension）

#### 如何提升可解释性

文章提出的方法是使用稀疏正则化和剪枝技术从较大的KAN开始训练，剪枝后的KAN比未剪枝的KAN更易解释。

![](./assets/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-05-17%20161855.png)

![](./assets/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-05-17%20162025.png)


